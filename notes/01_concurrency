------------------------------------------------------------
CHAPTER 1 - CONCURRENCY
------------------------------------------------------------

- Concurrent vs Parallel

    Parallel programs have a number of processing flows (mainly CPUs and cores) working independently 
      all at once.

    Concurrent programs have different processing flows (mostly threads) accessing and using a shared 
      resource at the same time.  Since this shared resource can be read and overwritten by any of the 
      different processing flows, some form of coordination is required at times, when the tasks that 
      need to be executed are not entirely independent from one another.



- Some tasks are 'inherently sequential', because the execution of some tasks depends heavily on the 
    results of others. In other words, those tasks are not independent, and thus, cannot be made parallel 
    or concurrent. Furthermore, if we were to try to implement concurrency into those programs, it could 
    cost us more execution time to produce the same results.

  Common examples include Newton's method and iterative numerical approximation algorithms.



- Some tasks are 'embarrassingly parallel', because they can be divided into different parallel tasks, 
    between which there is little or no dependency or need for communication.

  Common examples include graphics rendering and password cracking.



- Concurrency often cannot speed up IO-bound applications, since CPU computation is not the bottleneck.



- History of Concurrency

    - The term 'semaphore' originates from the 19th Century, when engineers were trying to design 
        ways of sharing railroad tracks and telegraph lines.  A semaphore is a system of sending messages 
        by holding the arms or two flags or poles in certain positions according to an alphabetic code.

    - A significant portion of the theoretical groundwork for concurrent programming was actually laid in 
        the 1960s. The early algorithmic language ALGOL 68, which was first developed in 1959, includes 
        features that support concurrent programming. 

    - The academic study of concurrency officially started with a seminal paper in 1965 from Edsger 
        Dijkstra.  That seminal paper is considered the first paper in the field of concurrent programming, 
        in which Dijkstra identified and solved the mutual exclusion problem. Mutual exclusion, which is a 
        property of concurrency control that prevents race conditions, went on to become one of the most 
        discussed topics in concurrency.

    - Yet, there was no considerable interest after that. From around 1970 to early 2000, processors were 
        said to double in executing speed every 18 months. During this period, programmers did not need to 
        concern themselves with concurrent programming, as all they had to do to have their programs run 
        faster was wait. 

    - However, in the early 2000s, a paradigm shift in the processor business took place; instead of making 
        increasingly big and fast processors for computers, manufacturers started focusing on smaller, slower 
        processors, which were put together in groups. This was when computers started to have multicore 
        processors.  Nowadays, an average computer has more than one core. So, if a programmer writes all of 
        their programs to be non-concurrent in any way, they will find that their programs utilize only one 
        core or one thread to process data, while the rest of the CPU sits idle, doing nothing. This is one 
        reason for the recent push in concurrent programming.

    - Another reason for the increasing popularity of concurrency is the growing field of graphical, multimedia, 
        and web-based application development, in which the application of concurrency is widely used to solve 
        complex and meaningful problems. For example, concurrency is a major player in web development: each new 
        request made by a user typically comes in as its own process (this is called multiprocessing) or 
        asynchronously coordinated with other requests (this is called asynchronous programming); if any of those 
        requests need to access a shared resource (a database, for example) where data can be changed, 
        concurrency should be taken into consideration.



- For years, some people have believed that compilers will come along that solve the concurrency problem, since
    many developers don't understand concurrency and it's considered a hard problem.  Such compilers would convert
    all statements into a dependency graph, then reorder and parallelize the statements.  

  However, such compilers do not yet exist.  They may in the future, but for now programmers need to understand
    concurrent programming techniques.